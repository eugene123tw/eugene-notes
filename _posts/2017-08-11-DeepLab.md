---
title: DeepLab - Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs
published: false
tags:
  - cv
---

## 3 components

### 1.

We remove the downsampling operator from the last few max pooling layers of DCNNs and instead upsample the filters in subsequent convolutional layers, resulting in feature maps computed at a higher sampling rate. Filter upsampling amounts to inserting holes (‘trous’ in French) between nonzero filter taps.

In practice, we recover full resolution feature maps by a combination of atrous convolution, which computes feature maps more densely, followed by simple bilinear interpolation of the feature responses to the original image size.

### 2. 

The second challenge is caused by the existence of objects at multiple scales. A standard way to deal with this is to present to the DCNN rescaled versions of the same image and then aggregate the feature or score maps [6], [17], [18]. We show that this approach indeed increases the performance of our system, but comes at the cost of computing feature responses at all DCNN layers for multiple scaled versions of the input image. 

Instead, motivated by spatial pyramid pooling [19], [20], we propose a computationally efficient scheme of resampling a given feature layer at multiple rates prior to convolution. Rather than actually resampling features, we efficiently implement this mapping using multiple parallel atrous convolutional layers with different sampling rates; we call the proposed technique “atrous spatial pyramid pooling” (ASPP).

### 3.

The third challenge relates to the fact that an object-centric classifier requires invariance to spatial transformations, inherently limiting the spatial accuracy of a DCNN. 

We boost our model’s ability to capture fine details by employing a **fully-connected Conditional Random Field (CRF) [22]**. CRFs have been broadly used in semantic segmentation to **combine** class scores computed by multi-way classifiers **with the low-level information captured by the local interactions of pixels and edges [23], [24] or superpixels [25].**

We use the fully connected pairwise CRF proposed by [22] for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies. That model was shown in [22] to improve the performance of a boosting-based pixel-level classifier. In this work, we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level classifier.

<img id="center" src="{{ site.baseurl }}/assets/img/2017-08-11-DeepLab/fig1.png">


## Arch

A deep convolutional neural network (e.g. VGG-16, ResNet-101) trained in the task of image classification is re-purposed to the task of semantic segmentation by 

* transforming all the fully connected layers to convolutional layers (i.e., fully convolutional network [14])

* increasing feature resolution through atrous convolutional layers, allowing us to compute feature responses every 8 pixels instead of every 32 pixels in the original network. We then employ bi-linear interpolation to upsample by a factor of 8 the score map to reach the original image resolution, yielding the input to a fully-connected CRF [22] that refines the segmentation results.


## Method

### 3.1 Atrous Convolution for Dense Feature Extraction and Field-of-View Enlargement

Considering one-dimensional signals first, the output $y[i]$ of atrous convolution 2 of a 1-D input signal $x[i]$ with a filter $w[k]$ of length $K$ is defined as:

$$ y[i] = \sum\limits^{K}_{k=1} x[i+r \cdot k]w[k] $$

<img id="center" src="{{ site.baseurl }}/assets/img/2017-08-11-DeepLab/fig2.png">


We illustrate the algorithm’s operation in 2-D through a simple example in Fig. 3: Given an image, we assume that we first have a downsampling operation that reduces the resolution by a factor of 2, and then perform a convolution with a kernel - here, the vertical Gaussian derivative. If one implants the resulting feature map in the original image coordinates, we realize that we have obtained responses at only 1/4 of the image positions.

<img id="center" src="{{ site.baseurl }}/assets/img/2017-08-11-DeepLab/fig3.png">

Instead, we can compute responses at all image positions if we convolve the full resolution image with a filter ‘with holes’, in which we up-sample the original filter by a factor of 2, and introduce zeros in between filter values.

We have adopted instead a hybrid approach that strikes a good efficiency/accuracy trade-off, using **atrous convolution to increase by a factor of 4 the density of computed feature maps, followed by fast bilinear interpolation by an additional factor of 8 to recover feature maps at the original image resolution**.

Atrous convolution also allows us to arbitrarily enlarge the **field-of-view** of filters at any DCNN layer. State-of-the- art DCNNs typically employ spatially small convolution kernels (typically $3×3$) in order to keep both computation and number of parameters contained. Atrous convolution with rate $r$ introduces $r − 1$ zeros between consecutive filter values, effectively enlarging the kernel size of a $×k$ filter to $k_{e} = k + (k − 1)(r − 1)$ without increasing the number of parameters or the amount of computation. 


### 3.2 Multiscale Image Representations using Atrous Spatial Pyramid Pooling

The second approach is inspired by the success of the R-CNN spatial pyramid pooling method of [20], which showed that regions of an arbitrary scale can be accurately and efficiently classified by resampling convolutional features extracted at a single scale. We have implemented a variant of their scheme which uses multiple parallel atrous convolutional layers with different sampling rates. The features extracted for each sampling rate are further processed in separate branches and fused to generate the final result. The proposed “atrous spatial pyramid pooling” (DeepLab- ASPP) approach generalizes our DeepLab-LargeFOV variant and is illustrated in Fig. 4.

<img id="center" src="{{ site.baseurl }}/assets/img/2017-08-11-DeepLab/fig4.png">

## 3.3 Structured Prediction with Fully-Connected Condi- tional Random Fields for Accurate Boundary Recovery

A trade-off between localization accuracy and classification performance seems to be inherent in DCNNs: deeper models with multiple max-pooling layers have proven most successful in classification tasks, however the increased in-variance and the large receptive fields of top-level nodes can only yield smooth responses.

<img id="center" src="{{ site.baseurl }}/assets/img/2017-08-11-DeepLab/fig5.png">

We pursue an alternative direction based on **coupling** the recognition capacity of DCNNs and the fine-grained localization accuracy of fully connected CRFs and show that it is remarkably successful in addressing the localization challenge, producing accurate semantic segmentation results and recovering object boundaries at a level of detail that is well beyond the reach of existing methods.

Traditionally, conditional random fields (CRFs) have been employed to smooth noisy segmentation maps [23], [31]. Typically these models couple neighboring nodes, favoring same-label assignments to spatially proximal pixels. Qualitatively, the primary function of these short-range CRFs is to clean up the spurious predictions of weak classifiers built on top of local hand-engineered features.

To overcome these limitations of short-range CRFs, we integrate into our system the fully connected CRF model of [22]. The model employs the energy function

$$ E(\mathcal{x})= \sum\limits_{i} \theta_{i}(x_{i})+\sum\limits_{ij}\theta_{ij}(x_{i},x_{j}) $$

where $\mathcal{x}$ is the label assignment for pixels. We use as unary potential $\theta_{i}(x_{i})=-\log P(x_{i})$, where $ P(x_{i})$ is the label assignment probability at pixel $i$ as computed by a DCNN.

The pairwise potential has a form that allows for efficient inference while using a fully-connected graph, i.e. when connecting all pairs of image pixels, $i,j$. In particular, as in [22], we use the following expression:

$$ \theta_{ij}(x_{i},x_{j})=\mu(x_{i},x_{j}) \left[ w_{1} \text{exp}(-\frac{||p_{i}-p_{j}||^{2}}{2\sigma^{2}_{\alpha}}-\frac{||I_{i}-I_{j}||^{2}}{2\sigma^{2}_{\beta}})+w_{2}\text{exp}(-\frac{||p_{i}-p_{j}||^{2}}{2\sigma^{2}_{\gamma}})  \right]  $$

where $\mu(x_{i}, x_{j})=1$ if $x_{i} \neq x_{j}$ , and zero otherwise, which, as in the Potts model, means that only nodes with distinct labels are penalized. The remaining expression uses two Gaussian kernels in different feature spaces; the first, ‘bilateral’ kernel depends on both pixel positions (denoted as $p$) and RGB color (denoted as $I$), and the second kernel only depends on pixel positions. The hyper parameters $\sigma_{\alpha}$, $\sigma_{\beta}$ and $\sigma_{\gamma}$ control the scale of Gaussian kernels. The first kernel forces pixels with similar color and position to have similar labels, while the second kernel only considers spatial proximity when enforcing smoothness.


## EXPERIMENTAL RESULTS
<img id="center" src="{{ site.baseurl }}/assets/img/2017-08-11-DeepLab/fig7.png">