---
title: YOLO 9000
published: false
tags:
  - Computer Vision
---

## Tutorial Repo

https://github.com/eugene123tw/pytorch_snippets/blob/master/YOLO.ipynb

## Direct location prediction

When using anchor boxes with YOLO we encounter a second issue: model instability, especially during early iterations. Most of the instability comes from predicting the (x, y) locations for the box. In region proposal networks the network predicts values $t_{x}$ and $t_{y}$ and the $(x, y)$ center coordinates are calculated as:

$$
\begin{gather}
x =(t_{x} \times w_{a})  − x_{a} \\
y = (t_{y} \times h_{a}) − y_{a}
\end{gather}
$$

For example, a prediction of $t_{x} = 1$ would shift the box to the right by the width of the anchor box ($w_{a}$), a prediction of $t_{x} = −1$ would shift it to the left by the same amount.

This formulation is unconstrained so any anchor box can end up at any point in the image, regardless of what loca- tion predicted the box. With random initialization the model takes a long time to stabilize to predicting sensible offsets.

Instead of predicting offsets we follow the approach of YOLO and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 0 and 1. We use a **logistic activation** to constrain the network’s predictions to fall in this range.

The network predicts 5 bounding boxes (`num=5`) at each cell in the output feature map. The network predicts 5 coordinates for each bounding box, $t_{x}, t_{y}, t_{w}, t_{h}$, and $t_{o}$. If the cell is offset from the top left corner of the image by $(c_{x},c_{y})$ and the bounding box prior has width and height $p_{w}, p_{h}$, then the predictions correspond to:

$$
\begin{align}
b_{x} &= \sigma(t_{x}) + c_{x} \\
b_{y} &= \sigma(t_{y}) + c_{y} \\
b_{w} &= p_{w} e^{t_{w}} \\
b_{h} &= p_{h} e^{t_{h}} \\
Pr(\text{object}) \times IOU(b,\text{object}) &= \sigma(t_{o})
\end{align}
$$

## Training for detection.

We modify this network for detection by removing the last convolutional layer and instead adding on three 3 × 3 convolutional layers with 1024 filters each followed by a final 1 × 1 convolutional layer with the number of outputs we need for detection. For VOC we predict 5 boxes with 5 coordinates each and 20 classes per box so 125 filters. We also add a passthrough layer from the final 3 × 3 × 512 layer to the second to last convolutional layer so that our model can use fine grain features.

## Route and Reorg layer

### Fine-Grained Features.

This modified YOLO predicts detections on a 13 × 13 feature map. While this is sufficient for large objects, it may benefit from finer grained features for localizing smaller objects. Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions. We take a different approach, simply adding a **passthrough layer**(route layer) that brings features from an earlier layer at 26 × 26 resolution.

The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in `ResNet`.

```py
elif d['type'] == '[route]': # add new layer here
    routes = d['layers']
    if type(routes) is int:
        routes = [routes]
    else:
        routes = [int(x.strip()) for x in routes.split(',')]
    routes = [i + x if x < 0 else x for x in routes]
    for j, x in enumerate(routes):
        lx = layers[x];
        xtype = lx['type']
        _size = lx['_size'][:3]
        if j == 0:
            h, w, c = _size
        else:
            h_, w_, c_ = _size
            assert w_ == w and h_ == h, \
            'Routing incompatible conv sizes'
            c += c_
    yield ['route', i, routes]
    l = w * h * c
#-----------------------------------------------------
elif d['type'] == '[reorg]':
    stride = d.get('stride', 1)
    yield ['reorg', i, stride]
    w = w // stride; h = h // stride;
    c = c * (stride ** 2)
    l = w * h * c
```

## Tutorials

**Code comes from the repo https://github.com/marvis/pytorch-yolo2.
The repo it's great! You can clone it and try it youself.**

# YOLO

It works by dividing an image into $S \times S$ grid. Each grid cell predicts $B$ bounding boxes and a confidence score for _each box_. The confidence scores show how confident the model is that there is an object in that bounding box. This is formally defined as:

$$ Confidence = Pr(object) \times \text{IOU}^{\text{predict}}_{\text{truth}} $$

In `yolo.cfg` model definitions, there are $80$ classes. Image input size of YOLO is $416 \times 416$. After $5$ times of down-sampling, the dimension of the feature map is $ 13 \times 13 \times 425 $. Why there are $425$ filters ? Since, there are $5$ anchor boxes, and we are predicting $5$ coordinates ($t_{x},t_{y},t_{w},t_{h},t_{o}$) for each anchor box and a class score for each classes at each cell for each anchor box.

For the setup in `yolo.cfg`, there are $5$ anchors boxes ($Anchor_{1},Anchor_{2},Anchor_{3},Anchor_{4},Anchor_{5}$). So, ($80$ class scores + $5$ coordinates) for each Anchor at each cell, the size of feature map: $ 13 \times 13 \times (85 \times 5)$. There will be $5+80$ one-dimensional array computed at $Anchor_{1}$, cell $(1,1)$. Size of $5$ to hold the predicted coordinates and $80$ to hold the class scores.

|      Anchor boxes      |          |          |          |          |          |          |          |          |          |
| :--------------------: | :------- | :------- | :------- | :------- | :------- | :------: | :------: | :------: | :------: |
|  Cell 1, $Anchor_{1}$  | $t_{x}$  | $t_{y}$  | $t_{w}$  | $t_{h}$  | $t_{o}$  | $ s_{0}$ | $s_{1}$  | $s_{2}$  | $\cdots$ | $s_{79}$ |
|  Cell 1, $Anchor_{2}$  | $t_{x}$  | $t_{y}$  | $t_{w}$  | $t_{h}$  | $t_{o}$  | $ s_{0}$ | $s_{1}$  | $s_{2}$  | $\cdots$ | $s_{79}$ |
|        $\vdots$        | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| Cell 169, $Anchor_{5}$ | $t_{x}$  | $t_{y}$  | $t_{w}$  | $t_{h}$  | $t_{o}$  | $ s_{0}$ | $s_{1}$  | $s_{2}$  | $\cdots$ | $s_{79}$ |

So, the dimension of feature maps $ 13 \times 13 $, $5$ anchor boxes on each cells. There are $ 13 \times 13 \times 5 = 845 $ anchor boxes in total. For each anchor box at each cell there is a vector with $85$ variables ($5$ coordinates + $80$ class score) . The total dimensional: $ 85 \times 845 $.

```python
import torch
import numpy as np
from torch.autograd import Variable
import math

# load output.npy array
output = np.load("output.npy")
print("Size of feature maps: ")
print("batch_size x filter_size x height x width = {}x{}x{}x{}".format(output.shape[0],output.shape[1],output.shape[2],output.shape[3]))
output = torch.from_numpy(output).cuda()
```

    Size of feature maps:
    batch_size x filter_size x height x width = 1x425x13x13

```python
conf_thresh = 0.5
num_classes = 80
anchors = [0.57273,0.677385, 1.87446,2.06253,3.33843,5.47434,7.88282,3.52778,9.77052,9.16828]
num_anchors = 5
only_objectness=1
validation=False
```

```python
anchor_step = int(len(anchors)/num_anchors)
if output.dim() == 3:
    output = output.unsqueeze(0)
batch = output.size(0)
assert(output.size(1) == (5+num_classes)*num_anchors)
h = output.size(2)
w = output.size(3)

all_boxes = []
output = output.view(batch*num_anchors, 5+num_classes, h*w).transpose(0,1).contiguous().view(5+num_classes, batch*num_anchors*h*w)
```

- Row size: 845
- Column size: 5 coordinates predictor + 80 class scores

|      Anchor boxes      |          |          |          |          |          |          |          |          |          |
| :--------------------: | :------- | :------- | :------- | :------- | :------- | :------: | :------: | :------: | :------: |
|  Cell 1, $Anchor_{1}$  | $t_{x}$  | $t_{y}$  | $t_{w}$  | $t_{h}$  | $t_{o}$  | $ s_{0}$ | $s_{1}$  | $s_{2}$  | $\cdots$ | $s_{79}$ |
|  Cell 1, $Anchor_{2}$  | $t_{x}$  | $t_{y}$  | $t_{w}$  | $t_{h}$  | $t_{o}$  | $ s_{0}$ | $s_{1}$  | $s_{2}$  | $\cdots$ | $s_{79}$ |
|        $\vdots$        | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| Cell 169, $Anchor_{5}$ | $t_{x}$  | $t_{y}$  | $t_{w}$  | $t_{h}$  | $t_{o}$  | $ s_{0}$ | $s_{1}$  | $s_{2}$  | $\cdots$ | $s_{79}$ |

```python
print("value x filter_size = {}x{}".format(output.shape[0],output.shape[1]))
```

    value x filter_size = 85x845

```python
# grid_x used to store the shift for each anchor box
torch.linspace(0, w-1, w).repeat(h,1).repeat(batch*num_anchors, 1, 1)
```

    (0 ,.,.) =
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12

    (1 ,.,.) =
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12

    (2 ,.,.) =
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12

    (3 ,.,.) =
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12

    (4 ,.,.) =
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
       0   1   2   3   4   5   6   7   8   9  10  11  12
    [torch.FloatTensor of size 5x13x13]

```python
# grid_y used to store the shift for each anchor box
torch.linspace(0, h-1, h).repeat(w,1).t().repeat(batch*num_anchors, 1, 1)
```

    (0 ,.,.) =
       0   0   0   0   0   0   0   0   0   0   0   0   0
       1   1   1   1   1   1   1   1   1   1   1   1   1
       2   2   2   2   2   2   2   2   2   2   2   2   2
       3   3   3   3   3   3   3   3   3   3   3   3   3
       4   4   4   4   4   4   4   4   4   4   4   4   4
       5   5   5   5   5   5   5   5   5   5   5   5   5
       6   6   6   6   6   6   6   6   6   6   6   6   6
       7   7   7   7   7   7   7   7   7   7   7   7   7
       8   8   8   8   8   8   8   8   8   8   8   8   8
       9   9   9   9   9   9   9   9   9   9   9   9   9
      10  10  10  10  10  10  10  10  10  10  10  10  10
      11  11  11  11  11  11  11  11  11  11  11  11  11
      12  12  12  12  12  12  12  12  12  12  12  12  12

    (1 ,.,.) =
       0   0   0   0   0   0   0   0   0   0   0   0   0
       1   1   1   1   1   1   1   1   1   1   1   1   1
       2   2   2   2   2   2   2   2   2   2   2   2   2
       3   3   3   3   3   3   3   3   3   3   3   3   3
       4   4   4   4   4   4   4   4   4   4   4   4   4
       5   5   5   5   5   5   5   5   5   5   5   5   5
       6   6   6   6   6   6   6   6   6   6   6   6   6
       7   7   7   7   7   7   7   7   7   7   7   7   7
       8   8   8   8   8   8   8   8   8   8   8   8   8
       9   9   9   9   9   9   9   9   9   9   9   9   9
      10  10  10  10  10  10  10  10  10  10  10  10  10
      11  11  11  11  11  11  11  11  11  11  11  11  11
      12  12  12  12  12  12  12  12  12  12  12  12  12

    (2 ,.,.) =
       0   0   0   0   0   0   0   0   0   0   0   0   0
       1   1   1   1   1   1   1   1   1   1   1   1   1
       2   2   2   2   2   2   2   2   2   2   2   2   2
       3   3   3   3   3   3   3   3   3   3   3   3   3
       4   4   4   4   4   4   4   4   4   4   4   4   4
       5   5   5   5   5   5   5   5   5   5   5   5   5
       6   6   6   6   6   6   6   6   6   6   6   6   6
       7   7   7   7   7   7   7   7   7   7   7   7   7
       8   8   8   8   8   8   8   8   8   8   8   8   8
       9   9   9   9   9   9   9   9   9   9   9   9   9
      10  10  10  10  10  10  10  10  10  10  10  10  10
      11  11  11  11  11  11  11  11  11  11  11  11  11
      12  12  12  12  12  12  12  12  12  12  12  12  12

    (3 ,.,.) =
       0   0   0   0   0   0   0   0   0   0   0   0   0
       1   1   1   1   1   1   1   1   1   1   1   1   1
       2   2   2   2   2   2   2   2   2   2   2   2   2
       3   3   3   3   3   3   3   3   3   3   3   3   3
       4   4   4   4   4   4   4   4   4   4   4   4   4
       5   5   5   5   5   5   5   5   5   5   5   5   5
       6   6   6   6   6   6   6   6   6   6   6   6   6
       7   7   7   7   7   7   7   7   7   7   7   7   7
       8   8   8   8   8   8   8   8   8   8   8   8   8
       9   9   9   9   9   9   9   9   9   9   9   9   9
      10  10  10  10  10  10  10  10  10  10  10  10  10
      11  11  11  11  11  11  11  11  11  11  11  11  11
      12  12  12  12  12  12  12  12  12  12  12  12  12

    (4 ,.,.) =
       0   0   0   0   0   0   0   0   0   0   0   0   0
       1   1   1   1   1   1   1   1   1   1   1   1   1
       2   2   2   2   2   2   2   2   2   2   2   2   2
       3   3   3   3   3   3   3   3   3   3   3   3   3
       4   4   4   4   4   4   4   4   4   4   4   4   4
       5   5   5   5   5   5   5   5   5   5   5   5   5
       6   6   6   6   6   6   6   6   6   6   6   6   6
       7   7   7   7   7   7   7   7   7   7   7   7   7
       8   8   8   8   8   8   8   8   8   8   8   8   8
       9   9   9   9   9   9   9   9   9   9   9   9   9
      10  10  10  10  10  10  10  10  10  10  10  10  10
      11  11  11  11  11  11  11  11  11  11  11  11  11
      12  12  12  12  12  12  12  12  12  12  12  12  12
    [torch.FloatTensor of size 5x13x13]

```python
grid_x = torch.linspace(0, w-1, w).repeat(h,1).repeat(batch*num_anchors, 1, 1).view(batch*num_anchors*h*w).cuda()
grid_y = torch.linspace(0, h-1, h).repeat(w,1).t().repeat(batch*num_anchors, 1, 1).view(batch*num_anchors*h*w).cuda()
```

**Compute bounding box center $(b_{x},b_{y})$: **

$$
\begin{align}
b_{x} &= \sigma(t_{x}) + c_{x} \\
b_{y} &= \sigma(t_{y}) + c_{y}
\end{align}
$$

```python
xs = torch.sigmoid(output[0]) + grid_x
print("Total number of x coordianates for all anchors: {}".format(xs.size()))
```

    Total number of x coordianates for all anchors: torch.Size([845])

```python
ys = torch.sigmoid(output[1]) + grid_y
print("Total number of y coordianates for all anchors: {}".format(ys.size()))
```

    Total number of y coordianates for all anchors: torch.Size([845])

```python
# reshape anchors to num_anchors * anchor_step
torch.Tensor(anchors).view(num_anchors, anchor_step)
```

     0.5727  0.6774
     1.8745  2.0625
     3.3384  5.4743
     7.8828  3.5278
     9.7705  9.1683
    [torch.FloatTensor of size 5x2]

```python
# Select along dim=1, and index = [0] -> That is select column:0, as width value
anchor_w = torch.Tensor(anchors).view(num_anchors, anchor_step).index_select(1, torch.LongTensor([0]))
anchor_w
```

     0.5727
     1.8745
     3.3384
     7.8828
     9.7705
    [torch.FloatTensor of size 5x1]

```python
# Select along dim=1, and index = [1] -> That is select column:1, as height value
anchor_h = torch.Tensor(anchors).view(num_anchors, anchor_step).index_select(1, torch.LongTensor([1]))
anchor_h
```

     0.6774
     2.0625
     5.4743
     3.5278
     9.1683
    [torch.FloatTensor of size 5x1]

```python
# There are 5 anchor boxes with 5 different width, creating 5 boxes on each cell (1*5*13*13)
anchor_w = anchor_w.repeat(batch, 1).repeat(1, 1, h*w).view(batch*num_anchors*h*w).cuda()
# There are 5 anchor boxes with 5 different height, creating 5 box boxes on each cell (1*5*13*13)
anchor_h = anchor_h.repeat(batch, 1).repeat(1, 1, h*w).view(batch*num_anchors*h*w).cuda()
```

**Compute bounding box width, height $(b_{w},b_{h})$: **

$$
\begin{align}
b_{w} &=  e^{t_{x}} p_{w} \\
b_{h} &=  e^{t_{y}} p_{h}
\end{align}
$$

```python
ws = torch.exp(output[2]) * anchor_w
hs = torch.exp(output[3]) * anchor_h
```

**Extract confidence score:**
$$ P r(object) \times IOU(b, object) = \sigma(t_{o}) $$

```python
det_confs = torch.sigmoid(output[4])
```

```python
cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data
cls_confs
```

     4.7196e-01  3.8000e-03  2.5265e-01  ...   9.7222e-04  6.5787e-05  1.6038e-04
     3.7180e-01  4.6476e-03  8.0054e-02  ...   3.3170e-03  1.7993e-04  2.6547e-04
     3.3778e-01  2.6349e-03  1.6774e-01  ...   1.2523e-03  6.7661e-05  2.7283e-04
                    ...                   ⋱                   ...
     3.1134e-01  9.6280e-03  1.3979e-02  ...   3.1822e-03  1.2804e-03  4.4047e-03
     9.8989e-02  1.5729e-02  1.7123e-02  ...   6.2427e-03  1.8017e-03  7.1307e-03
     7.0600e-02  1.4053e-02  3.9510e-02  ...   8.8036e-03  3.1567e-03  8.1155e-03
    [torch.cuda.FloatTensor of size 845x80 (GPU 0)]

```python
# searching for the max conf for each cell
cls_max_confs, cls_max_ids = torch.max(cls_confs, 1)
```

```python
print("Shape of cls_max_confs: {}".format(cls_max_confs.size()))
```

    Shape of cls_max_confs: torch.Size([845])

```python
print("Shape of cls_max_ids: {}".format(cls_max_ids.size()))
```

    Shape of cls_max_ids: torch.Size([845])

```python
def convert2cpu(gpu_matrix):
    return torch.FloatTensor(gpu_matrix.size()).copy_(gpu_matrix)

def convert2cpu_long(gpu_matrix):
    return torch.LongTensor(gpu_matrix.size()).copy_(gpu_matrix)


def load_class_names(namesfile):
    class_names = []
    with open(namesfile, 'r') as fp:
        lines = fp.readlines()
    for line in lines:
        line = line.rstrip()
        class_names.append(line)
    return class_names

def nms(boxes, nms_thresh):
    if len(boxes) == 0:
        return boxes

    det_confs = torch.zeros(len(boxes))
    for i in range(len(boxes)):
        det_confs[i] = 1-boxes[i][4]

    _,sortIds = torch.sort(det_confs)
    out_boxes = []
    for i in range(len(boxes)):
        box_i = boxes[sortIds[i]]
        if box_i[4] > 0:
            out_boxes.append(box_i)
            for j in range(i+1, len(boxes)):
                box_j = boxes[sortIds[j]]
                if bbox_iou(box_i, box_j, x1y1x2y2=False) > nms_thresh:
                    #print(box_i, box_j, bbox_iou(box_i, box_j, x1y1x2y2=False))
                    box_j[4] = 0
    return out_boxes

def bbox_iou(box1, box2, x1y1x2y2=True):
    if x1y1x2y2:
        mx = min(box1[0], box2[0])
        Mx = max(box1[2], box2[2])
        my = min(box1[1], box2[1])
        My = max(box1[3], box2[3])
        w1 = box1[2] - box1[0]
        h1 = box1[3] - box1[1]
        w2 = box2[2] - box2[0]
        h2 = box2[3] - box2[1]
    else:
        mx = min(box1[0]-box1[2]/2.0, box2[0]-box2[2]/2.0)
        Mx = max(box1[0]+box1[2]/2.0, box2[0]+box2[2]/2.0)
        my = min(box1[1]-box1[3]/2.0, box2[1]-box2[3]/2.0)
        My = max(box1[1]+box1[3]/2.0, box2[1]+box2[3]/2.0)
        w1 = box1[2]
        h1 = box1[3]
        w2 = box2[2]
        h2 = box2[3]
    uw = Mx - mx
    uh = My - my
    cw = w1 + w2 - uw
    ch = h1 + h2 - uh
    carea = 0
    if cw <= 0 or ch <= 0:
        return 0.0

    area1 = w1 * h1
    area2 = w2 * h2
    carea = cw * ch
    uarea = area1 + area2 - carea
    return carea/uarea
```

```python
sz_hw         = h*w                            # total cells per anchor
sz_hwa        = sz_hw*num_anchors              # total anchors

# Convert cuda variable to gpu variable -----
det_confs     = convert2cpu(det_confs)
cls_max_confs = convert2cpu(cls_max_confs)
cls_max_ids   = convert2cpu_long(cls_max_ids)
xs            = convert2cpu(xs)
ys            = convert2cpu(ys)
ws            = convert2cpu(ws)
hs            = convert2cpu(hs)
# -------------------------------------------

if validation:
    cls_confs = convert2cpu(cls_confs.view(-1, num_classes))


for b in range(batch):
    boxes = []
    for cy in range(h):
        for cx in range(w):
            for i in range(num_anchors):
                """
                --------------------------------------------------
                Since all the anchors are flatten to one dimension,
                we need a formula to help us find the right index,
                for the right cell.

                There are 169 cells per Anchor (5 anchors in total),
                if we want to compute the object confidence at cell(0,0) for each anchor.

                Formula: b*sz_hwa + i*sz_hw + cy*w + cx
                Reminder: Since we are only testing with 1 img, so batch=1, b=0

                --------------------------------------------------
                Ex: cell[0,0]
                Anchor0-ind = 0*845+0*169+0*13+0 = [0]
                Anchor1-ind = 0*845+1*169+1*13+0 = [169]
                Anchor2-ind = 0*845+2*169+1*13+0 = [169]
                --------------------------------------------------
                Ex: cell[5,0]
                Anchor0-ind = 0*845 + 0*169 + 5*13 + 0 = [65]
                Anchor1-ind = 0*845 + 1*169 + 5*13 + 0 = [234]
                Anchor2-ind = 0*845 + 2*169 + 5*13 + 0 = [403]
                --------------------------------------------------
                Ex: cell[5,13]
                Anchor0-ind = 0*845 + 0*169 + 5*13 + 13 = [78]
                Anchor1-ind = 0*845 + 1*169 + 5*13 + 13 = [247]
                Anchor2-ind = 0*845 + 2*169 + 5*13 + 13 = [416]
                """

                ind = b*sz_hwa + i*sz_hw + cy*w + cx
                det_conf =  det_confs[ind]
                if only_objectness:
                    conf =  det_confs[ind]
                else:
                    conf = det_confs[ind] * cls_max_confs[ind]

                if conf > conf_thresh:
                    bcx = xs[ind]
                    bcy = ys[ind]
                    bw = ws[ind]
                    bh = hs[ind]
                    cls_max_conf = cls_max_confs[ind]
                    cls_max_id = cls_max_ids[ind]
                    """
                    bcx: bbox center x
                    bcy: bbox center y
                    bw : bbox width
                    bh : bbox height

                    bcx/w: (bbox center x) divided (width of feature map) to extracted relative ratio between 0~1
                    bcy/h: (bbox center y) divided (width of feature map) to extracted relative ratio between 0~1
                    bw/w : (bbox width  y) divided (width of feature map) to extracted relative ratio between 0~1
                    bh/h : (bbox height y) divided (width of feature map) to extracted relative ratio between 0~1
                    """
                    box = [bcx/w, bcy/h, bw/w, bh/h, det_conf, cls_max_conf, cls_max_id]
                    if (not only_objectness) and validation:
                        for c in range(num_classes):
                            tmp_conf = cls_confs[ind][c]
                            if c != cls_max_id and det_confs[ind]*tmp_conf > conf_thresh:
                                box.append(tmp_conf)
                                box.append(c)
                    boxes.append(box)
    all_boxes.append(boxes)

all_boxes = all_boxes[0]
```

# Detection: plot_boxes

```python
from PIL import Image, ImageDraw

def plot_boxes(img, boxes, savename=None, class_names=None):
    colors = torch.FloatTensor([[1,0,1],[0,0,1],[0,1,1],[0,1,0],[1,1,0],[1,0,0]]);
    def get_color(c, x, max_val):
        ratio = float(x)/max_val * 5
        i = int(math.floor(ratio))
        j = int(math.ceil(ratio))
        ratio = ratio - i
        r = (1-ratio) * colors[i][c] + ratio*colors[j][c]
        return int(r*255)

    width = img.width
    height = img.height
    draw = ImageDraw.Draw(img)
    for i in range(len(boxes)):
        box = boxes[i]
        x1 = (box[0] - box[2]/2.0) * width
        y1 = (box[1] - box[3]/2.0) * height
        x2 = (box[0] + box[2]/2.0) * width
        y2 = (box[1] + box[3]/2.0) * height

        rgb = (255, 0, 0)
        if len(box) >= 7 and class_names:
            cls_conf = box[5]
            cls_id = box[6]
            print('%s: %f' % (class_names[cls_id], cls_conf))
            classes = len(class_names)
            offset = cls_id * 123457 % classes
            red   = get_color(2, offset, classes)
            green = get_color(1, offset, classes)
            blue  = get_color(0, offset, classes)
            rgb = (red, green, blue)
            draw.text((x1, y1), class_names[cls_id], fill=rgb)
        draw.rectangle([x1, y1, x2, y2], outline = rgb)
    if savename:
        print("save plot results to %s" % savename)
        img.save(savename)
    return img

img = Image.open("dog.jpg").convert('RGB')
class_names = load_class_names('coco.names')
all_boxes = nms(all_boxes, nms_thresh=0.4)
plot_boxes(img, all_boxes,'predictions.jpg', class_names)
```

    truck: 0.908544
    bicycle: 0.997499
    dog: 0.993630
    save plot results to predictions.jpg

![png]({{ site.baseurl }}/assets/img/2017-06-14-YOLO-9000/YOLO_files/YOLO_29_1.png)
