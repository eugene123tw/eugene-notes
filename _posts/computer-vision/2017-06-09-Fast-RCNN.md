---
title: RCNN
published: false
tags:
  - cv
---

# R-CNN

<div class="card mb-3">
    <img class="card-img-top" src="{{ site.baseurl }}/assets/img/2017-06-09-Fast-RCNN/RCNN.jpg"/>
</div>

<!-- more -->

## Architecture of RCNN

1. Object Proposal:  
   R-CNN will create 2k object propasals(Region Proposals).  
   Object proposals are created by applying Selective Search or by applying Edge Boxes.

2. For each object proposals Crop + Wrap:

- Resize the object proposals to 227 x 227 image
- Forward each cropped/wrapped object proposals and save pool5 features.

3. (Classification Head) Train binary SVM per class to classify each object proposal

4. (Regression Head) Train linear regression model to map from cached features to offsets boxes to make up for slightly wrong proposals

# SPP-Net

![R-CNN]({{ site.baseurl }}/assets/img/2017-06-09-Fast-RCNN/SPPNET.jpg "SPPNET")

## Solved

1. Fixed image size problem:  
   In this paper, they introduce a spatial pyramid pooling (SPP) [14], [15] layer to remove the fixed-size constraint of the network, (since the requirement of fixed sizes is only due to the fully-connected layers that demand fixed-length vectors as inputs). This is done by add an SPP layer on top of the last convolutional layer.

> SPP layer: The SPP layer pools the features and generates fixed length outputs, which are then fed into the fully connected layers. This can be illustrated as [image -> conv layers -> SPP layer -> FC layers -> output].

> Implementation of SPP:

1. SPP is able to generate a fixed length output regardless of the input size, while the sliding window pooling used in the previous deep networks cannot.
2. SPP uses multi-level spatial bins, while the sliding window pooling uses only a single window size.
3. SPP can pool features extracted at variable scales thanks to the flexibility of input scales.

## Training with different size images

Training with variable-size images increases scale-invariance and reduces over-fitting. We develop a simple multi-size training method. For a single network to accept variable input sizes, we approximate it by multiple networks that share all parameters, while each of these networks is trained using a fixed input size. In each epoch we train the network with a given input size, and switch to another input size for the next epoch. Experiments show that this multi-size training converges just as the traditional single-size training, and leads to better testing accuracy.

## Architecture of SPP-Net

# Fast R-CNN

## Architecture of Fast-RCNN

1. A Fast R-CNN network takes as input an entire image and a set of object proposals.

The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters that are independent of any particular RoI. In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple (r, c, h, w) that specifies its top-left corner (r, c) and its height and width (h, w).

RoI (r, c, h, w) ---> RoI pooling layer ----> feature map of H × W (e.g., 7 × 7)

> Note: ROI (r, c, h, w) specifies its top-left corner (r, c) and its height and width (h, w).

## Feature maps

![R-CNN]({{ site.baseurl }}/assets/img/2017-06-09-Fast-RCNN/feature_map.png "FEATUREMAP")

## Differentiable ROI Pooling

![ROI Pooling]({{ site.baseurl }}/assets/img/2017-06-09-Fast-RCNN/ROI_pooling.png "ROI Pooling")

RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].

## Fine-tuning for detection

In Fast R-CNN training, (SGD) mini-batches are sampled hierarchically, first by sampling N images and then by sampling R/N RoIs from each image. Critically, RoIs from the same image share computation and memory in the forward and backward passes. Making N small decreases mini-batch computation. For example, when using N = 2 and R = 128, the proposed training scheme is roughly 64× faster than sampling one RoI from 128 different images (i.e., the R-CNN and SPPnet strategy)

# Multi-task loss.

A Fast R-CNN network has two sibling output layers.

- The first outputs a discrete probability distribution (per RoI),$ p=(p_{0},...,p_{K}) $,over $K+1$ categories(`scores = blobs_out['cls_prob']`). As usual, $p$ is computed by a softmax over the $K+1$ outputs of a fully connected layer.

- The second sibling layer outputs bounding-box regression offsets, $t^{k} = ( t_{x}^{k},t_{y}^{k},t_{w}^{k},t^{k}_{h})$ (`box_deltas = blobs_out['bbox_pred']`), for each of the $K$ object classes, indexed by $k$. We use the parameterization for $t^{k}$ given in [9], in which $t^{k}$ specifies a scale-invariant translation and log-space height/width shift relative to an object proposal.

<style>
#center {
    margin-left: 150px;
    width: 300px;
}
</style>
